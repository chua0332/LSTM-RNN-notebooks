{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Directional LSTM exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-Directional LSTMs focus on the problem of getting the most out of the input sequence by stepping through input time steps in both forward and backward directions. In practice, this means that the neural network architecture invovles duplicating the first recurrent layer in the network so that there are now two layers side by side, providing the input sequence as input to the first layer and providing a reversed copy of the input sequence to the second.\n",
    "\n",
    "This approach was developed some time ago as a general approach for improving the performance of Recurrent Neural Networks (RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach has been used to great effect with LSTM Recurrent Neural Networks. Providing the entire sequence both forwards and backwards is based on the assumption that entire sequence is made available. The use of providing an input sequence bi-directionally is justified in the domain of speech recognition because there is evidence that in humans, the context of the whole utterance is used to interpret what is being said rather than a linear interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Keras Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#model = Sequential()\n",
    "#model.add(Bidirectional(LSTM(...),input_shape(timesteps,features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional LSTMs are supported in Keras via the Bidirectional layer wrapper that essentially merges the output from two parallel LSTMs, one with input processed forward and one with outputprocessed backwards. Thius wrapper takes a recurrent layer **(the first LSTM layer)** as an argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Bidirectional** wrapper layer allows me to specify the **merge** mode. That is how the forward and backward outputs should be combined before being passed on to the next layer.\n",
    "\n",
    "* *'sum'*: The outputs are added together.\n",
    "* *'mul'*: The outputs are multiplied together.\n",
    "* *concat'*: The outputs are concatenated together (default behavior), hence providing double the number of outputs to the next layer.\n",
    "* *'ave'*: The average of the outputs is taken\n",
    "\n",
    "The default model is to just concatenate, and this method is frequently used in the studies of Bi-Directional LSTMs. However in general, it might be a good idea to test each of the merge modes on your problem to see if you can improve upon the concatenate option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cumulative sum sequence\n",
    "def get_sequence(n_timesteps):\n",
    "    X = np.array([np.random.random_sample() for _ in range(n_timesteps)])\n",
    "  # calculate cut-off value to change class values\n",
    "    limit = n_timesteps/4.0\n",
    "  # determine the class outcome for each item in cumulative sequence\n",
    "    y = np.array([0 if x < limit else 1 for x in np.cumsum(X)])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multiple samples of cumulative sum sequences\n",
    "def get_sequences(n_sequences, n_timesteps):\n",
    "    seqX, seqY = list(), list()\n",
    "  # create and store sequences\n",
    "    for _ in range(n_sequences):\n",
    "        X, y = get_sequence(n_timesteps)\n",
    "        seqX.append(X)\n",
    "        seqY.append(y)\n",
    "  # reshape input and output for lstm\n",
    "    seqX = np.array(seqX).reshape(n_sequences, n_timesteps, 1)\n",
    "    seqY = np.array(seqY).reshape(n_sequences, n_timesteps, 1)\n",
    "    return seqX, seqY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see whether the self defined functions work in generating the sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.33992229]\n",
      "  [0.49760616]\n",
      "  [0.32990696]\n",
      "  [0.94534795]\n",
      "  [0.92051324]\n",
      "  [0.88647614]\n",
      "  [0.30372564]\n",
      "  [0.16002192]\n",
      "  [0.17838608]\n",
      "  [0.04900133]]\n",
      "\n",
      " [[0.02202858]\n",
      "  [0.72145108]\n",
      "  [0.49023022]\n",
      "  [0.21037025]\n",
      "  [0.83273424]\n",
      "  [0.71582997]\n",
      "  [0.77922793]\n",
      "  [0.86375578]\n",
      "  [0.02266436]\n",
      "  [0.07758068]]]\n",
      "[[[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [1]\n",
      "  [1]\n",
      "  [1]\n",
      "  [1]\n",
      "  [1]\n",
      "  [1]]\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [1]\n",
      "  [1]\n",
      "  [1]\n",
      "  [1]\n",
      "  [1]]]\n"
     ]
    }
   ],
   "source": [
    "X,y = get_sequences(2,10)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 10, 100)           20800     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 10, 1)             101       \n",
      "=================================================================\n",
      "Total params: 20,901\n",
      "Trainable params: 20,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define problem\n",
    "n_timesteps = 10\n",
    "# define LSTM\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, return_sequences=True), input_shape=(n_timesteps, 1))) \n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid'))) \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "We can use the **get_sequence()** function to generate a large number of random examples on which to fit the model. We can generate a large number of examples, in this case 40,000, store them in memory, and then fit them in one Keras epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "40000/40000 [==============================] - 24s 610us/step - loss: 0.0256 - acc: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1369c5080>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train LSTM\n",
    "X, y = get_sequences(40000, n_timesteps)\n",
    "model.fit(X, y, epochs=1, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "We can evaluate the model by generating 100 new random sequences and calculating the accuracy of the predictions made by the fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.017961, Accuracy: 99.400002\n"
     ]
    }
   ],
   "source": [
    "#evaluate LSTM\n",
    "X, y = get_sequences(100,n_timesteps)\n",
    "loss, acc = model.evaluate(X ,y, verbose=0)\n",
    "print('Loss: %f, Accuracy: %f' % (loss, acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above code prints both the log loss and accuracy. We can see that the model achieves close to 100% accuracy :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=[0 0 0 0 0 1 1 1 1 1], yhat=[0 0 0 0 0 1 1 1 1 1], correct=True\n",
      "y=[0 0 0 0 0 0 1 1 1 1], yhat=[0 0 0 0 0 0 1 1 1 1], correct=True\n",
      "y=[0 0 0 1 1 1 1 1 1 1], yhat=[0 0 0 1 1 1 1 1 1 1], correct=True\n",
      "y=[0 0 0 0 0 1 1 1 1 1], yhat=[0 0 0 0 0 1 1 1 1 1], correct=True\n",
      "y=[0 0 0 0 0 0 0 0 1 1], yhat=[0 0 0 0 0 0 0 0 1 1], correct=True\n",
      "y=[0 0 0 0 1 1 1 1 1 1], yhat=[0 0 0 0 1 1 1 1 1 1], correct=True\n",
      "y=[0 0 0 1 1 1 1 1 1 1], yhat=[0 0 0 1 1 1 1 1 1 1], correct=True\n",
      "y=[0 0 0 0 0 0 1 1 1 1], yhat=[0 0 0 0 0 0 1 1 1 1], correct=True\n",
      "y=[0 0 0 0 1 1 1 1 1 1], yhat=[0 0 0 0 1 1 1 1 1 1], correct=True\n",
      "y=[0 0 0 1 1 1 1 1 1 1], yhat=[0 0 0 1 1 1 1 1 1 1], correct=True\n",
      "y=[0 0 0 0 0 0 1 1 1 1], yhat=[0 0 0 0 0 0 1 1 1 1], correct=True\n",
      "y=[0 0 0 0 1 1 1 1 1 1], yhat=[0 0 0 0 1 1 1 1 1 1], correct=True\n",
      "y=[0 0 0 1 1 1 1 1 1 1], yhat=[0 0 0 1 1 1 1 1 1 1], correct=True\n",
      "y=[0 0 0 0 0 1 1 1 1 1], yhat=[0 0 0 0 0 1 1 1 1 1], correct=True\n",
      "y=[0 0 0 1 1 1 1 1 1 1], yhat=[0 0 0 1 1 1 1 1 1 1], correct=True\n",
      "y=[0 0 0 1 1 1 1 1 1 1], yhat=[0 0 0 1 1 1 1 1 1 1], correct=True\n",
      "y=[0 0 0 0 1 1 1 1 1 1], yhat=[0 0 0 0 1 1 1 1 1 1], correct=True\n",
      "y=[0 0 0 0 1 1 1 1 1 1], yhat=[0 0 0 0 1 1 1 1 1 1], correct=True\n",
      "y=[0 0 0 0 0 0 0 1 1 1], yhat=[0 0 0 0 0 0 0 1 1 1], correct=True\n",
      "y=[0 0 0 0 0 0 1 1 1 1], yhat=[0 0 0 0 0 0 1 1 1 1], correct=True\n"
     ]
    }
   ],
   "source": [
    "## Making predictions from the model\n",
    "\n",
    "for _ in range(20):\n",
    "    X, y = get_sequences(1,n_timesteps)\n",
    "    yhat = model.predict_classes(X, verbose=0)\n",
    "    exp, pred = y.reshape(n_timesteps), yhat.reshape(n_timesteps)\n",
    "    print('y=%s, yhat=%s, correct=%s' % (exp, pred, np.array_equal(exp,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
